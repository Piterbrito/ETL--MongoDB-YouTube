# ETL-YouTube-data-INTO-Cloud-Database
The goal is to extract the data from a reliable data source like Kaggle and bring it into the python environment with pandas as a csv and structure it into a pandas dataframe to begin the transformation phase of the data by cleaning the data, fixing the null and missing values , grouping by relevant variables to create visualizations and identify the outlets with maximum sales and the other trends and variables contributing to those sales. After the data is cleaned and fixed i will load the pandas dataframe into a local database such as postgreSQL and check for SQL tables with SQL commands.

Data Cleanup & Analysis:-

Once I have identified the datasets,I will perform ETL on the data and document the following within the jpynb :-

The sources of data that I will extract from.

The type of transformation needed for this data (cleaning, joining, filtering, aggregating, etc).

The type of final production database to load the data into (relational or non-relational).

The final tables or collections that will be used in the production database.

Submit a final technical report with the above information and steps required to reproduce your ETL process.

Along with this I have included a detailed data dictionary along with the code and the corresponding output of each cell step by step that will cover the detailed explanantion of taking that particular approach towards solving the problem.
